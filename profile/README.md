## 🪄 Prompirit: Prompt Editing AI Tool to Improve AI-Generative Work Using User Emotion

#### 💗 23-2, 24-1 이화여자대학교 캡스톤디자인 🎨 18팀 뽀로로에디

| <img src="https://github.com/AnT-Prompirit/.github/assets/77625287/648828be-460b-481b-88a5-a1af54166983" width="400"> | <img src="https://github.com/AnT-Prompirit/.github/assets/77625287/2efbffaf-fbc6-4adf-bffc-de5568a2fada" width="400"> | <img src="https://github.com/AnT-Prompirit/.github/assets/77625287/43c610b8-6076-4c2f-a972-c061fbea7f32" width="380"> |
| ----------------------------------- | ---------------------------------------| ------------------------------------- |
|[김한나 Hannah Kim](https://github.com/gamddalki)|[이현 Hyun Lee](https://github.com/hyuni0316)|[방선유 Sunyu Bang](https://github.com/syou-b)|
|팀장, 텍스트 감정 분석, 감정 라벨 추가, LMER 피팅 통계|텍스트 토큰화, 스타일 키워드, 평균 & 정규화 통계, 이미지 생성|스타일 키워드, 설문지 제작, ANOVA & Pearson 통계|

## 🌟 Abstract
최근 발전한 text-to-image 생성형 AI 모델들은 간단한 텍스트 프롬프트를 이용해 고품질의 이미지를 만들어내는 능력을 보여주고 있다. 그러나 아직까지는 생성형 AI가 감정을 정확하게 반영한 이미지를 만들어내는 능력은 부족하다. 따라서 우리는 감정의 표현력과 미학성을 개선하기 위한 자동 프롬프트 엔지니어링인 **Prompirit**을 제안한다. 우리는 사용자가 입력하는 자유로운 형식의 텍스트 입력을 개선하기 위해 사용자의 감정과 스타일 키워드를 포함하는 다양한 접근법을 탐구했다. 통계적 분석과 100명의 응답자를 대상으로 한 사용자 평가에서 Prompirit는 이미지-감정의 일치도와 생성된 이미지의 미학성을 현저히 향상시키는 동시에 원본 텍스트 입력의 내용을 정확하게 전달하는 것으로 나타났다. 이러한 결과를 바탕으로, 우리는 프롬피릿이 감정을 잘 표현하는 이미지를 효과적으로 생성할 수 있음을 시사한다.

## 📃 Full Paper
#### [🪄 Prompirit: Prompt Editing AI Tool to Improve AI-Generative Work Using User Emotion](https://sites.google.com/view/prompirit-pororoeddy)
<img width="1731" alt="KakaoTalk_20240614_231354646" src="https://github.com/AnT-Prompirit/.github/assets/77625287/2e81bbd7-2f70-4964-b9ad-7a73974b9b82">

Example images generated by DALL·E 3 given an original input text (_original_) and three other prompt engineering methods: _LLaMA-2_, _Reprompt_, and **_Prompirit_** (ours). The red and blue text refer to emotion label and style modifiers suggested by Prompirit, respectively.

#### Prompt engineering pipeline of Prompirit
![pipeline](https://github.com/AnT-Prompirit/.github/assets/77625287/f35313a2-f524-4c0d-a77e-7723994eac96)


## 💻 Source Code
#### [👩🏻‍🔬 Prompirit의 MAIN PROCESS 실험 코드 모음](https://github.com/AnT-Prompirit/experiment)
#### [💌 Prompirit의 프롬프트 엔지니어링 파이프라인 DEMO](https://github.com/AnT-Prompirit/prompirit_final_code)
#### [🪄 Prompirit이 적용된 웹사이트 프로토타입](https://github.com/AnT-Prompirit/Prompirit)


## ⚙️ Tech
<img src="https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=Python&logoColor=white"> <img src="https://img.shields.io/badge/PyTorch-EE4C2C?style=for-the-badge&logo=PyTorch&logoColor=white"> <img src="https://img.shields.io/badge/OpenAI-412991?style=for-the-badge&logo=OpenAI&logoColor=white">   ![React](https://img.shields.io/badge/React-61DAFB.svg?style=for-the-badge&logo=React&logoColor=fff)

##
#### References
[1] Yunlong Wang, Shuyuan Shen, and Brian Y. Lim. 2023. RePrompt: Automatic Prompt Editing to Refine AI-Generative Art Towards Precise Expressions. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI ’23), April 23–28, 2023, Hamburg, Germany. ACM, New York, NY, USA, 29 pages. https://doi.org/10.1145/3544548.3581402

[2] Jonas Oppenlaender. 2022. Prompt Engineering for Text-Based Generative Art. arXiv 1, 1. Retrieved from http://arxiv.org/abs/2204.13988

[3] Ankita Gandhi, Kinjal Adhvaryu, Soujanya Poria, Erik Cambria, Amir Hussain, Multimodal sentiment analysis: A systematic review of history, datasets, multimodal fusion methods, applications, challenges and future directions, Information Fusion, Volume 91, 2023, Pages 424-444, ISSN 1566-2535, https://doi.org/10.1016/j.inffus.2022.09.025.

[4] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. 2017. Tensor Fusion Network for Multimodal Sentiment Analysis. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1103–1114, Copenhagen, Denmark. Association for Computational Linguistics.

[5] Andrea Scarantino. 2017. How to Do Things with Emotional Expressions: The Theory of Affective Pragmatics. Psychological Inquiry 28, 2–3: 165–185. https://doi.org/10.1080/1047840X.2017.1328951
